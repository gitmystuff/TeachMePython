{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/TeachMePython/blob/main/Worked_Examples/Search_Agent_by_Sam_Witteveen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Search Agent By Sam Witteveen\n",
        "\n",
        "https://www.youtube.com/watch?v=MkqkiJgnDxk\n",
        "\n",
        "This example uses an OPENAI_API_KEY and TAVILY_API_KEY."
      ],
      "metadata": {
        "id": "j7XsdocrVQ9Y"
      },
      "id": "j7XsdocrVQ9Y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pydantic AI\n",
        "\n",
        "Pydantic AI is a Python framework designed to simplify the creation of production-grade applications that utilize Generative AI. It leverages the data validation capabilities of Pydantic to ensure structured and reliable responses from Large Language Models (LLMs).\n",
        "\n",
        "Here's a breakdown of what that means:\n",
        "\n",
        "* **Agent Framework:** It provides the tools and structure to build AI agents, which are systems that can perceive their environment, make decisions, and take actions to achieve a goal.\n",
        "* **Built on Pydantic:** It tightly integrates with Pydantic, a popular Python library for data validation and parsing. This means you can define the expected structure and data types of the inputs and outputs of your AI agents using Pydantic models.\n",
        "* **Type-Safe AI Applications:** By using Pydantic's type hinting and validation, Pydantic AI helps ensure that the data exchanged with LLMs is in the expected format, leading to more robust and less error-prone AI applications.\n",
        "* **Structured Responses:** A key advantage is its ability to guide LLMs to return structured data (e.g., JSON) that conforms to your Pydantic models. This makes it easier to work with the LLM's output in your Python code.\n",
        "* **Model Agnostic:** Pydantic AI is designed to work with various LLM providers (like OpenAI, Anthropic, Google Gemini, etc.), offering a consistent way to interact with them.\n",
        "* **Tools and Dependency Injection:** It allows you to define \"tools\" (functions that the AI agent can use) and provides a system for dependency injection, making it easier to manage and test your AI applications.\n",
        "* **Integration with Pydantic Logfire:** It seamlessly integrates with Pydantic Logfire for real-time debugging, performance monitoring, and behavior tracking of your LLM-powered applications.\n",
        "\n",
        "In essence, Pydantic AI aims to bring the clarity and robustness of Pydantic's data handling to the world of Generative AI application development, similar to how FastAPI brought those principles to web development."
      ],
      "metadata": {
        "id": "2KsrVQUeV6Fl"
      },
      "id": "2KsrVQUeV6Fl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tavily\n",
        "\n",
        "Tavily AI is a company that provides AI-powered tools, primarily a **Search API**, designed to enhance the capabilities of Large Language Models (LLMs) and AI agents by giving them access to real-time and accurate information from the web.\n",
        "\n",
        "Here's a breakdown of what Tavily offers:\n",
        "\n",
        "* **AI-Optimized Search:** Tavily's search engine is specifically built for LLMs and Retrieval-Augmented Generation (RAG) systems. It aims to provide more relevant and factual results compared to traditional search engines for AI applications.\n",
        "* **Real-time Web Access:** It allows AI agents to access up-to-date information, which is crucial for tasks requiring current data.\n",
        "* **Comprehensive Research:** Tavily can scrape, filter, and aggregate data from multiple web sources to provide more comprehensive answers.\n",
        "* **Focus on Accuracy and Credibility:** It emphasizes providing reliable information from trusted sources, which can help reduce hallucinations in LLMs.\n",
        "* **Tools for Data Extraction:** Besides search, Tavily also offers tools to extract specific content from web pages.\n",
        "* **Integration with LLM Frameworks:** Tavily is designed to integrate easily with popular LLM frameworks like LangChain.\n",
        "\n",
        "In simple terms, Tavily helps LLMs \"see\" and understand the latest information on the internet more effectively, making them more knowledgeable and reliable.\n",
        "\n",
        "You can think of it as a specialized search engine that understands the needs of AI models."
      ],
      "metadata": {
        "id": "8pSIbVnaWJo3"
      },
      "id": "8pSIbVnaWJo3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pydantic Base Model\n",
        "\n",
        "A **Pydantic Base Model** is the fundamental building block in the Pydantic library for data validation and parsing in Python. It's a class that you define, inheriting from `pydantic.BaseModel`, to represent a data structure.\n",
        "\n",
        "Here's a breakdown of its key characteristics and purpose:\n",
        "\n",
        "  * **Data Structure Definition:** You define a Pydantic Base Model by creating a Python class that inherits from `pydantic.BaseModel`. The class attributes you declare within this model define the fields of your data structure.\n",
        "  * **Type Hinting:** You use Python type hints for these class attributes to specify the expected data type for each field (e.g., `str`, `int`, `float`, `List[str]`, other Pydantic models, etc.).\n",
        "  * **Data Validation:** When you create an instance of a Pydantic Base Model with some input data (e.g., a dictionary, JSON), Pydantic automatically validates this data against the type hints you've defined.\n",
        "      * If the data conforms to the types, Pydantic parses it and creates an instance of your model.\n",
        "      * If the data doesn't conform, Pydantic raises a `ValidationError` providing detailed information about the validation errors.\n",
        "  * **Data Parsing/Serialization:** Pydantic automatically tries to coerce the input data into the expected types. For example, a string `\"123\"` might be automatically converted to the integer `123` if the corresponding field is type-hinted as `int`. It also provides methods for serializing the model instance back into dictionaries or JSON.\n",
        "  * **Data Access:** Once you have a valid instance of a Pydantic Base Model, you can access its fields as regular Python object attributes (e.g., `my_model.name`).\n",
        "\n",
        "**In essence, a Pydantic Base Model allows you to:**\n",
        "\n",
        "1.  **Clearly define the expected structure and types of your data.**\n",
        "2.  **Ensure that incoming data conforms to this structure through automatic validation.**\n",
        "3.  **Easily parse and work with the validated data as Python objects.**\n",
        "\n",
        "**Example:**\n",
        "\n",
        "```python\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class ResearchResult(BaseModel):\n",
        "    research_title: str = Field(description='Top-level Markdown heading summarizing the query topic, prefixed with \"#\".')\n",
        "    research_main: str = Field(description='Main body providing detailed answers to the query and research findings.')\n",
        "    research_bullets: str = Field(description='Concise bullet-point summary of the key answers derived from the research.')\n",
        "```\n",
        "\n",
        "Let's break down this Pydantic `ResearchResult` class:\n",
        "\n",
        "This class, `ResearchResult`, inherits from `pydantic.BaseModel`. This means it leverages Pydantic's capabilities for data validation and parsing. It's designed to structure the output of some research process, likely performed by an AI agent or a similar system.\n",
        "\n",
        "Here's an explanation of each field:\n",
        "\n",
        "  * **`research_title: str = Field(description='...')`**:\n",
        "\n",
        "      * `research_title` is a field of type `str` (string).\n",
        "      * `Field(...)` is used to provide additional metadata about this field.\n",
        "      * `description='Top-level Markdown heading summarizing the query topic, prefixed with \"#\".'` This description explains that the `research_title` should be a string that acts as the main title for the research results. It should be formatted as a top-level Markdown heading (starting with `#`) and should summarize the topic of the initial query and the subsequent research.\n",
        "\n",
        "  * **`research_main: str = Field(description='...')`**:\n",
        "\n",
        "      * `research_main` is also a field of type `str`.\n",
        "      * `description='Main body providing detailed answers to the query and research findings.'` This indicates that `research_main` should contain the primary content of the research output. It should provide comprehensive answers to the original query, incorporating the information gathered through the research process.\n",
        "\n",
        "  * **`research_bullets: str = Field(description='...')`**:\n",
        "\n",
        "      * `research_bullets` is another string field.\n",
        "      * `description='Concise bullet-point summary of the key answers derived from the research.'` This suggests that `research_bullets` should be a string containing a summary of the most important findings or answers from the research, formatted as bullet points (typically using `*` or `-` in Markdown).\n",
        "\n",
        "In Pydantic, `Field` is a function used within the definition of a `BaseModel` to provide **more control and metadata** about a specific field (attribute) of the model. While you can define a field simply by its name and type annotation (e.g., `name: str`), `Field` allows you to specify additional properties.\n",
        "\n",
        "**Purpose of `Field`:**\n",
        "\n",
        "1.  **Provide a default value:** Similar to how you'd assign a default in a regular Python class attribute definition.\n",
        "2.  **Add metadata:** You can attach extra information to the field, which can be useful for documentation, UI generation, or custom validation.\n",
        "3.  **Specify validation constraints:** While Pydantic's type hints already enforce basic type validation, `Field` allows you to add more specific constraints (though Pydantic's validation features have expanded beyond just `Field` for this).\n",
        "\n",
        "**Common Parameters of `Field`:**\n",
        "\n",
        "  * **`default`**: The default value for the field if no value is provided during instance creation.\n",
        "  * **`alias`**: A different name that can be used when receiving data (e.g., from JSON). This allows you to map external data structures to your Pydantic model field names.\n",
        "  * **`title`**: A human-readable title for the field, often used in documentation or UI forms.\n",
        "  * **`description`**: A more detailed explanation of the field's purpose.\n",
        "  * **`const`**: If set to `True`, the field's value cannot be changed after the instance is created.\n",
        "  * **Validation-related parameters (some have moved to other Pydantic features in newer versions, but you might still see them):**\n",
        "      * `gt`, `ge`, `lt`, `le`: Greater than, greater than or equal to, less than, less than or equal to (for numeric types).\n",
        "      * `min_length`, `max_length`: Minimum and maximum length (for strings).\n",
        "      * `regex`: A regular expression the string must match.\n",
        "\n",
        "In summary, an instance of the `ResearchResult` class is expected to hold the outcome of a research process, structured into a main title, a detailed body, and a bullet-point summary, all as strings. Pydantic will ensure that when you create an instance of `ResearchResult`, the values provided for these fields are strings (or can be coerced into strings)."
      ],
      "metadata": {
        "id": "DVyYsGjzYCyh"
      },
      "id": "DVyYsGjzYCyh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataclass\n",
        "\n",
        "In Python's `dataclasses` library, a **dataclass** is a class that is primarily designed to hold data. The `dataclass` decorator automatically generates several \"boilerplate\" methods for you, reducing the amount of code you need to write. These automatically generated methods typically include:\n",
        "\n",
        "  * `__init__`: Initializes the instance with the provided values for the fields.\n",
        "  * `__repr__`: Provides a readable string representation of the object.\n",
        "  * `__eq__`: Allows you to compare two instances for equality based on their fields.\n",
        "  * `__hash__`: Enables instances to be used in sets and as dictionary keys (if the dataclass is frozen).\n",
        "  * `__lt__`, `__le__`, `__gt__`, `__ge__`: Support for comparison operators (if `order=True` is specified in the decorator).\n",
        "\n",
        "To create a dataclass, you import the `dataclass` decorator and apply it to a class. You then define the attributes of the class with type hints.\n",
        "\n",
        "**Key Features of Dataclasses:**\n",
        "\n",
        "  * **Concise Syntax:** They allow you to define data-holding classes with less code compared to traditional classes.\n",
        "  * **Automatic Method Generation:** The decorator handles the creation of common methods, making your code cleaner.\n",
        "  * **Type Hinting:** Type hints are crucial for defining the fields of a dataclass.\n",
        "  * **Mutability:** By default, dataclass instances are mutable (their attributes can be changed). You can make them immutable by using the `frozen=True` argument in the `@dataclass` decorator.\n",
        "\n",
        "the `@dataclass` decorator automatically creates the `__init__`, `__repr__`, and `__eq__` methods for the `Point` class.\n",
        "\n",
        "**Comparison with Pydantic Base Models (briefly):**\n",
        "\n",
        "While both dataclasses and Pydantic Base Models are used to represent data, Pydantic goes a step further by providing robust **data validation** and **parsing**. Pydantic models ensure that the data conforms to the specified types and can automatically coerce data types. Dataclasses primarily focus on being a convenient way to create data-holding classes with automatically generated methods.\n",
        "\n",
        "When you need data validation and parsing, Pydantic Base Models are generally preferred. When you just need a simple container for data with automatically generated common methods, dataclasses can be sufficient.\n"
      ],
      "metadata": {
        "id": "wzy1mORoYbBD"
      },
      "id": "wzy1mORoYbBD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "289e1acc-bfab-4a3d-a862-2b4a85e37a64",
      "metadata": {
        "id": "289e1acc-bfab-4a3d-a862-2b4a85e37a64"
      },
      "outputs": [],
      "source": [
        "# pip install tavily-python devtools -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Code"
      ],
      "metadata": {
        "id": "qAmlT-wEbih6"
      },
      "id": "qAmlT-wEbih6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de929277-405d-4226-812d-943af0c89722",
      "metadata": {
        "id": "de929277-405d-4226-812d-943af0c89722"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "from pydantic_ai import Agent, ModelRetry, RunContext\n",
        "from pydantic import BaseModel, Field\n",
        "from tavily import TavilyClient, AsyncTavilyClient\n",
        "from dataclasses import dataclass\n",
        "from typing import Any\n",
        "from devtools import debug\n",
        "from httpx import AsyncClient\n",
        "import datetime\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from dotenv import load_dotenv\n",
        "import logfire\n",
        "logfire.configure(send_to_logfire='if-token-present')\n",
        "\n",
        "load_dotenv()\n",
        "nest_asyncio.apply()\n",
        "tavily_client = AsyncTavilyClient()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "834bc3c0-f979-4654-832e-d765d4288b62",
      "metadata": {
        "id": "834bc3c0-f979-4654-832e-d765d4288b62"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class SearchDataclass:\n",
        "    max_results: int\n",
        "    todays_date: str\n",
        "\n",
        "@dataclass\n",
        "class ResearchDependencies:\n",
        "    todays_date: str\n",
        "\n",
        "class ResearchResult(BaseModel):\n",
        "    research_title: str = Field(description='Top-level Markdown heading summarizing the query topic, prefixed with \"#\".')\n",
        "    research_main: str = Field(description='Main body providing detailed answers to the query and research findings.')\n",
        "    research_bullets: str = Field(description='Concise bullet-point summary of the key answers derived from the research.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41367d52-6245-416a-90b8-5b5c3eba4eb7",
      "metadata": {
        "id": "41367d52-6245-416a-90b8-5b5c3eba4eb7"
      },
      "outputs": [],
      "source": [
        "search_agent = Agent('gpt-4o',\n",
        "                     deps_type=ResearchDependencies,\n",
        "                     result_type=ResearchResult,\n",
        "                     system_prompt='Your a helpful research assistant, you are an expert in research '\n",
        "                     'When given a query, write 3 - 5 keyword searches '\n",
        "                     '(each with a query_number) and then combine the results' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eeca9e4-ddb2-4d75-8d2a-711a2c41ccfe",
      "metadata": {
        "id": "4eeca9e4-ddb2-4d75-8d2a-711a2c41ccfe"
      },
      "outputs": [],
      "source": [
        "@search_agent.tool\n",
        "async def get_search(search_data:RunContext[SearchDataclass],query: str, query_number: int) -> dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Get the search for a keyword query.\n",
        "    Args:\n",
        "        query: keywords to search.\n",
        "    \"\"\"\n",
        "    print(f\"Search query {query_number}: {query}\")\n",
        "    max_results = search_data.deps.max_results\n",
        "    results = await tavily_client.get_search_context(query=query, max_results=max_results)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1198b9d8-b65a-46c3-85e7-3c417a11d11d",
      "metadata": {
        "id": "1198b9d8-b65a-46c3-85e7-3c417a11d11d",
        "outputId": "17535b73-48c0-4732-e6b6-26451f724ed7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "07:32:33.285 search_agent run prompt=Please give me a detailed definition of data science.\n",
            "07:32:33.286   preparing model request params run_step=1\n",
            "07:32:33.287   model request\n",
            "07:32:34.365   handle model response\n",
            "07:32:34.366     running tools=['get_search', 'get_search', 'get_search']\n",
            "Search query 1: data science definition\n",
            "Search query 2: what is data science\n",
            "Search query 3: data science explained\n",
            "07:32:38.342   preparing model request params run_step=2\n",
            "07:32:38.342   model request\n",
            "07:32:44.128   handle model response\n"
          ]
        }
      ],
      "source": [
        "# dependencies and search\n",
        "tavily_client = AsyncTavilyClient()\n",
        "current_date = datetime.date.today()\n",
        "date_string = current_date.strftime(\"%Y-%m-%d\")\n",
        "deps = SearchDataclass(max_results=3, todays_date=date_string)\n",
        "\n",
        "result = await search_agent.run(\n",
        "    \"Please give me a detailed definition of data science.\", deps=deps\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "863518b8-5dd7-4d0a-bfc5-026134acd86e",
      "metadata": {
        "id": "863518b8-5dd7-4d0a-bfc5-026134acd86e",
        "outputId": "57c8f29c-bee1-4a17-caba-cdf36cefec6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "research_title='Understanding Data Science' research_main='Data science is an interdisciplinary field that merges mathematics, statistics, programming, advanced analytics, machine learning, and artificial intelligence to extract insights and knowledge from data. It applies various techniques and algorithms to analyze large amounts of information, deriving actionable insights that help drive decision-making in various domains. Data science goes beyond traditional statistical analysis by incorporating computational tools to automate processes, make predictions, and generate insights.\\n\\nThe key components of data science include data collection, data wrangling, data analysis, and data visualization. This field leverages tools such as Python, R, Jupyter notebooks, SQL, machine learning libraries like SciPy and scikit-learn, and cloud-based platforms that democratize data access.\\n\\nData science is widely applicable, being utilized across numerous industries, including healthcare, finance, marketing, and more. It is especially valuable for its ability to uncover hidden patterns and build predictive models that enhance strategic initiatives across organizations. As such, data science continues to grow in importance and demand, fueled by its capacity to unlock the potential within data.\\n\\nTo embark on a career in data science, acquiring skills in programming, data analysis, machine learning, and domain-specific knowledge is essential. Platforms like IBM Cloud and various educational resources provide tools and courses geared towards developing proficiency in data science skills and methodologies.' research_bullets='- Data science is an interdisciplinary field involving math, statistics, programming, and AI.\\n- It aims to extract insights and knowledge from structured and unstructured data.\\n- Techniques include data wrangling, data analysis, machine learning, and data visualization.\\n- Widely used in industries like healthcare, finance, and marketing to drive decision-making.\\n- Tools and languages commonly used include Python, R, SQL, and machine learning libraries.\\n- Data science leverages computational tools to automate analysis and generate insights.\\n- The field offers growing career opportunities, demanding skills in programming and analysis.'\n"
          ]
        }
      ],
      "source": [
        "print(result.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcbd31b5-5bbc-4341-9889-db2693f2b83f",
      "metadata": {
        "id": "bcbd31b5-5bbc-4341-9889-db2693f2b83f",
        "outputId": "27115ca1-7076-4a90-b0b2-44c289f7443a"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "# Understanding Data Science\n",
              "\n",
              "Data science is an interdisciplinary field that merges mathematics, statistics, programming, advanced analytics, machine learning, and artificial intelligence to extract insights and knowledge from data. It applies various techniques and algorithms to analyze large amounts of information, deriving actionable insights that help drive decision-making in various domains. Data science goes beyond traditional statistical analysis by incorporating computational tools to automate processes, make predictions, and generate insights.\n",
              "\n",
              "The key components of data science include data collection, data wrangling, data analysis, and data visualization. This field leverages tools such as Python, R, Jupyter notebooks, SQL, machine learning libraries like SciPy and scikit-learn, and cloud-based platforms that democratize data access.\n",
              "\n",
              "Data science is widely applicable, being utilized across numerous industries, including healthcare, finance, marketing, and more. It is especially valuable for its ability to uncover hidden patterns and build predictive models that enhance strategic initiatives across organizations. As such, data science continues to grow in importance and demand, fueled by its capacity to unlock the potential within data.\n",
              "\n",
              "To embark on a career in data science, acquiring skills in programming, data analysis, machine learning, and domain-specific knowledge is essential. Platforms like IBM Cloud and various educational resources provide tools and courses geared towards developing proficiency in data science skills and methodologies.\n",
              "\n",
              "- Data science is an interdisciplinary field involving math, statistics, programming, and AI.\n",
              "- It aims to extract insights and knowledge from structured and unstructured data.\n",
              "- Techniques include data wrangling, data analysis, machine learning, and data visualization.\n",
              "- Widely used in industries like healthcare, finance, and marketing to drive decision-making.\n",
              "- Tools and languages commonly used include Python, R, SQL, and machine learning libraries.\n",
              "- Data science leverages computational tools to automate analysis and generate insights.\n",
              "- The field offers growing career opportunities, demanding skills in programming and analysis."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.data.research_title = '## '+result.data.research_title\n",
        "search_result = \"\\n\\n\".join([result.data.research_title, result.data.research_main, result.data.research_bullets])\n",
        "Markdown(search_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "659dfee6-26d8-4def-aaac-a7079621f925",
      "metadata": {
        "id": "659dfee6-26d8-4def-aaac-a7079621f925"
      },
      "outputs": [],
      "source": [
        "@search_agent.system_prompt\n",
        "async def add_current_date(ctx: RunContext[ResearchDependencies]) -> str:\n",
        "    todays_date = ctx.deps.todays_date\n",
        "    system_prompt=f'Your a helpful research assistant, you are an expert in research \\\n",
        "                If you are given a question you write strong keywords to do 3-5 searches in total \\\n",
        "                (each with a query_number) and then combine the results \\\n",
        "                if you need todays date it is {todays_date}'\n",
        "    return system_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71ed9c28-e467-439a-b8f5-0f0ade946cd2",
      "metadata": {
        "id": "71ed9c28-e467-439a-b8f5-0f0ade946cd2",
        "outputId": "57638964-63c0-4e7a-89a9-95970fc5e73c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "07:33:52.520 search_agent run prompt=What are the major AI News announcements in the last few days?\n",
            "07:33:52.521   preparing model request params run_step=1\n",
            "07:33:52.521   model request\n",
            "07:33:53.606   handle model response\n",
            "07:33:53.607     running tools=['get_search', 'get_search', 'get_search']\n",
            "Search query 1: major AI news announcements May 2025\n",
            "Search query 2: recent AI breakthroughs May 2025\n",
            "Search query 3: AI industry news latest May 2025\n",
            "07:33:58.070   preparing model request params run_step=2\n",
            "07:33:58.070   model request\n",
            "07:34:05.870   handle model response\n"
          ]
        }
      ],
      "source": [
        "result = await search_agent.run(\n",
        "    'What are the major AI News announcements in the last few days?', deps=deps\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "611c0c24-8d00-45e9-b6ac-ad16e42ca4be",
      "metadata": {
        "id": "611c0c24-8d00-45e9-b6ac-ad16e42ca4be",
        "outputId": "eb92b357-cab3-4d0c-ff31-41de78e5eb67"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "## Recent AI News and Announcements - May 2025\n",
              "\n",
              "In May 2025, several major announcements and breakthroughs in AI technology were highlighted across prominent tech events and industry leaders. At the forefront, Microsoft’s Build 2025 event on May 19 introduced significant advancements in their AI offerings, including improvements in Copilot AI and new AI agents designed to revolutionize how users interact with technology across various platforms. They are focusing on integrating AI more deeply into their ecosystem, notably with Windows 11 and Dynamics 365.\n",
              "\n",
              "Simultaneously, Nvidia made headlines with its presentations at Computex 2025 and ICRA, unveiling groundbreaking progress in AI-driven robotics and infrastructure. This includes advancements in multimodal generative AI and synthetic data generation that are expected to push the boundaries of robotics and automation.\n",
              "\n",
              "Additionally, a compelling challenge to US tech giants was issued by Alibaba’s latest AI model, Qwen3, which has closed the technological gap with leading American AI developments. This hints at a continuing global race in AI technology, with companies like Microsoft, Nvidia, and Alibaba all driving innovation in the field.\n",
              "\n",
              "Beyond company-specific announcements, there is an increasing focus on AI safety and ethics, as noted by industry experts discussing the balance between product development and responsible AI usage. This reflects a broader industry trend toward safer AI applications as these technologies become more integrated into everyday operations.\n",
              "\n",
              "- Microsoft Build 2025, held on May 19, announced significant AI updates including Copilot upgrades and new AI agents.\n",
              "- Nvidia showcased advancements in multimodal generative AI and robotics at Computex 2025 and ICRA.\n",
              "- Alibaba's AI model Qwen3 narrows the gap with leading US AI technologies.\n",
              "- Industry experts emphasize AI safety and responsible usage amid rapid AI product development."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.data.research_title = '## '+result.data.research_title\n",
        "search_result = \"\\n\\n\".join([result.data.research_title, result.data.research_main, result.data.research_bullets])\n",
        "Markdown(search_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Typing Any\n",
        "\n",
        "The line `from typing import Any` in Python imports the `Any` type hint from the `typing` module.\n",
        "\n",
        "**What `Any` means:**\n",
        "\n",
        "The `Any` type hint indicates that a variable or function parameter can hold a value of **any type**. It essentially tells the type checker (like MyPy) that you are intentionally allowing flexibility in the type of data used at that point.\n",
        "\n",
        "**When is `Any` used?**\n",
        "\n",
        "1.  **When the exact type is unknown or can vary:** If a variable might hold a string in one instance and an integer in another, you might use `Any`.\n",
        "2.  **When interacting with code that doesn't have type hints:** You might use `Any` as a way to loosely type data coming from external libraries or older code.\n",
        "3.  **As a way to \"opt-out\" of type checking for a specific variable:** While generally discouraged in favor of more specific types, `Any` can be used if you don't want the type checker to enforce a particular type.\n",
        "\n",
        "**In the context of Pydantic:**\n",
        "\n",
        "You might see `Any` used as a type hint in a Pydantic model field when the type of that field is not strictly defined or can be flexible. However, it's generally better to use more specific types in Pydantic models to leverage its validation capabilities effectively. Overuse of `Any` can reduce the benefits of using Pydantic for data validation.\n",
        "\n",
        "Do you have any other questions about type hinting or the `typing` module?"
      ],
      "metadata": {
        "id": "QnVFaj26aIyJ"
      },
      "id": "QnVFaj26aIyJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Devtools\n",
        "\n",
        "The line `from devtools import debug` imports the `debug` function from the `devtools` library.\n",
        "\n",
        "**What `devtools.debug` does:**\n",
        "\n",
        "The `debug()` function from the `devtools` library is an incredibly helpful tool for inspecting variables and their values during the execution of your Python code. When you call `debug(your_variable)`, it will print a nicely formatted representation of that variable to your console, often providing more detail and clarity than a simple `print(your_variable)`.\n",
        "\n",
        "**How it helps:**\n",
        "\n",
        "1.  **Improved Readability:** The output of `debug()` is often easier to read, especially for complex data structures like nested dictionaries, lists of objects, etc. It typically formats the output in a more structured and visually appealing way.\n",
        "2.  **Detailed Inspection:** It can show you the type of the variable and the values of its attributes if it's an object. For collections, it clearly displays the elements.\n",
        "3.  **Easier Debugging:** When you're trying to understand the state of your program and the values of your variables at a certain point, `debug()` can give you a clearer picture than a basic `print()`. This makes it easier to identify unexpected values or data structures that might be causing issues.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Without `debug()`:\n",
        "\n",
        "```python\n",
        "my_data = {\n",
        "    \"name\": \"Example\",\n",
        "    \"values\": [1, 2, {\"a\": 10, \"b\": 20}],\n",
        "    \"active\": True\n",
        "}\n",
        "print(my_data)\n",
        "```\n",
        "\n",
        "Output:\n",
        "\n",
        "```\n",
        "{'name': 'Example', 'values': [1, 2, {'a': 10, 'b': 20}], 'active': True}\n",
        "```\n",
        "\n",
        "With `debug()`:\n",
        "\n",
        "```python\n",
        "from devtools import debug\n",
        "\n",
        "my_data = {\n",
        "    \"name\": \"Example\",\n",
        "    \"values\": [1, 2, {\"a\": 10, \"b\": 20}],\n",
        "    \"active\": True\n",
        "}\n",
        "debug(my_data)\n",
        "```\n",
        "\n",
        "Output (may vary slightly depending on your terminal):\n",
        "\n",
        "```\n",
        "{'name': 'Example',\n",
        " 'values': [1, 2, {'a': 10, 'b': 20}],\n",
        " 'active': True}\n",
        "```\n",
        "\n",
        "For more complex objects, the difference in readability becomes even more significant. `devtools.debug()` often includes syntax highlighting and more structured indentation.\n",
        "\n",
        "**In summary, importing `debug` from `devtools` gives you a more powerful and readable way to inspect the state of your variables during development and debugging, making it easier to understand what's going on in your code.**\n",
        "\n",
        "Have you used `devtools.debug()` before?"
      ],
      "metadata": {
        "id": "Y7CczwcPajZx"
      },
      "id": "Y7CczwcPajZx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HTTPX AsyncClient\n",
        "\n",
        "The line `from httpx import AsyncClient` imports the `AsyncClient` class from the `httpx` library.\n",
        "\n",
        "**What `httpx` is:**\n",
        "\n",
        "`httpx` is a modern, fully asynchronous HTTP client for Python. It's designed to be a drop-in replacement for the well-known `requests` library but with added benefits like:\n",
        "\n",
        "  * **Asynchronous Support:** It fully supports `async/await` for concurrent HTTP requests, making it efficient for I/O-bound operations.\n",
        "  * **HTTP/2 Support:** It can handle HTTP/2 in addition to HTTP/1.1.\n",
        "  * **Request and Response Objects:** It provides clear and powerful objects for representing HTTP requests and responses.\n",
        "  * **Connection Pooling:** It manages connections efficiently.\n",
        "  * **Standard Library Integration:** It works well with Python's standard typing features.\n",
        "\n",
        "**What `AsyncClient` does:**\n",
        "\n",
        "The `AsyncClient` class is the asynchronous client provided by `httpx`. You use it within `async` functions to make HTTP requests in a non-blocking way. This is crucial for building high-performance applications that need to make multiple network requests concurrently without freezing.\n",
        "\n",
        "**How it's used:**\n",
        "\n",
        "You would typically use `AsyncClient` within an `async` function using `async with` to ensure proper resource management (like closing connections).\n",
        "\n",
        "```python\n",
        "import asyncio\n",
        "from httpx import AsyncClient\n",
        "\n",
        "async def fetch_data(url: str):\n",
        "    async with AsyncClient() as client:\n",
        "        response = await client.get(url)\n",
        "        if response.status_code == 200:\n",
        "            print(f\"Data from {url}: {response.text[:50]}...\")\n",
        "        else:\n",
        "            print(f\"Failed to get {url}: {response.status_code}\")\n",
        "\n",
        "async def main():\n",
        "    await asyncio.gather(\n",
        "        fetch_data(\"https://www.example.com\"),\n",
        "        fetch_data(\"https://httpbin.org/delay/1\"),\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())\n",
        "```\n",
        "\n",
        "In this example:\n",
        "\n",
        "1.  We import `AsyncClient` from `httpx`.\n",
        "2.  In `fetch_data`, we create an instance of `AsyncClient` using `async with`.\n",
        "3.  We use `await client.get(url)` to make an asynchronous GET request. The `await` keyword means the function will pause here until the request is complete, but it won't block the entire event loop, allowing other asynchronous tasks to run.\n",
        "4.  The `main` function uses `asyncio.gather` to run two `fetch_data` calls concurrently.\n",
        "\n",
        "**In summary, `from httpx import AsyncClient` makes the asynchronous HTTP client from the `httpx` library available in your code, allowing you to perform non-blocking HTTP requests within asynchronous Python programs.**\n"
      ],
      "metadata": {
        "id": "0TyzU8e0a5_J"
      },
      "id": "0TyzU8e0a5_J"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Asyncio\n",
        "\n",
        "The `import asyncio` statement in Python makes the `asyncio` library available in your code. The `asyncio` library is fundamental for writing **concurrent code** using the `async/await` syntax. It provides the infrastructure for running and managing asynchronous operations.\n",
        "\n",
        "Here are some of the key things that `asyncio` provides:\n",
        "\n",
        "1.  **The Event Loop:** This is the heart of `asyncio`. The event loop manages and executes asynchronous tasks. You can think of it as a central orchestrator that decides what runs when.\n",
        "\n",
        "2.  **Coroutines (`async def`):** `asyncio` allows you to define coroutines using the `async def` syntax. These are functions that can be paused and resumed, allowing other code to run in the meantime.\n",
        "\n",
        "3.  **Tasks:** You can wrap coroutines in `Task` objects. Tasks are awaitable futures, and they represent a scheduled coroutine. `asyncio` provides functions like `asyncio.create_task()` to create tasks.\n",
        "\n",
        "4.  **Futures:** A `Future` represents the eventual result of an asynchronous operation. Tasks are a specific type of Future. You can `await` a Future to wait for its result.\n",
        "\n",
        "5.  **Synchronization Primitives:** `asyncio` offers tools for coordinating asynchronous tasks, such as:\n",
        "\n",
        "      * `asyncio.Lock`: For mutual exclusion.\n",
        "      * `asyncio.Semaphore`: For limiting access to a resource.\n",
        "      * `asyncio.Event`: For signaling between tasks.\n",
        "      * `asyncio.Queue`: For safely passing data between coroutines.\n",
        "\n",
        "6.  **Networking Support:** `asyncio` provides asynchronous support for network operations (TCP, UDP, SSL/TLS, subprocesses, etc.).\n",
        "\n",
        "7.  **Running Asynchronous Code:** It provides functions like `asyncio.run()` to easily run the event loop and your asynchronous code.\n",
        "\n",
        "**In essence, `import asyncio` gives you the tools to write non-blocking I/O-bound code, which is crucial for applications like web servers, network clients, and any program that needs to perform multiple operations concurrently without waiting for one to finish before starting the next.**\n",
        "\n",
        "**Example:**\n",
        "\n",
        "```python\n",
        "import asyncio\n",
        "\n",
        "async def greet(name):\n",
        "    print(f\"Hello, {name}!\")\n",
        "    await asyncio.sleep(1)  # Simulate an I/O-bound operation\n",
        "    print(f\"Goodbye, {name}!\")\n",
        "\n",
        "async def main():\n",
        "    task1 = asyncio.create_task(greet(\"Alice\"))\n",
        "    task2 = asyncio.create_task(greet(\"Bob\"))\n",
        "    await asyncio.gather(task1, task2)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())\n",
        "```\n",
        "\n",
        "In this example, `asyncio` allows `greet(\"Alice\")` and `greet(\"Bob\")` to run concurrently. The `await asyncio.sleep(1)` pauses each coroutine without blocking the other."
      ],
      "metadata": {
        "id": "3mi6Lm3BbS44"
      },
      "id": "3mi6Lm3BbS44"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nest_Asyncio\n",
        "\n",
        "The `import nest_asyncio` statement allows you to **run nested event loops** within an already running `asyncio` event loop.\n",
        "\n",
        "**Why is this needed?**\n",
        "\n",
        "Normally, `asyncio` doesn't allow you to start a new event loop if one is already running in the same thread. This can be problematic in certain scenarios, such as:\n",
        "\n",
        "1.  **Interactive environments:** When you're working in environments like Jupyter notebooks or interactive Python shells that might already have an event loop running.\n",
        "2.  **Libraries that manage their own event loops:** Some libraries you use might internally create and run their own `asyncio` event loops. If your main application is also using `asyncio`, you might run into conflicts.\n",
        "3.  **Testing:** When you want to test asynchronous code that relies on its own event loop within a test environment that might also be asynchronous.\n",
        "\n",
        "**What `nest_asyncio.apply()` does:**\n",
        "\n",
        "After importing `nest_asyncio`, you typically call `nest_asyncio.apply()`. This function patches the `asyncio` module to allow the creation of nested event loops.\n",
        "\n",
        "**In essence, `nest_asyncio` provides a workaround to the \"event loop already running\" error in `asyncio` by enabling the nesting of event loops.**\n",
        "\n",
        "**Example Scenario (Jupyter Notebook):**\n",
        "\n",
        "Without `nest_asyncio`, if you try to run an `asyncio` event loop within a Jupyter notebook cell where the IPython kernel already has one running, you might get an error.\n",
        "\n",
        "```python\n",
        "import asyncio\n",
        "\n",
        "async def main():\n",
        "    print(\"Running in the main event loop\")\n",
        "    await asyncio.sleep(1)\n",
        "    print(\"Main loop finished\")\n",
        "\n",
        "try:\n",
        "    asyncio.run(main())\n",
        "except RuntimeError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "```\n",
        "\n",
        "With `nest_asyncio`:\n",
        "\n",
        "```python\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "async def main():\n",
        "    print(\"Running in the nested event loop\")\n",
        "    await asyncio.sleep(1)\n",
        "    print(\"Nested loop finished\")\n",
        "\n",
        "asyncio.run(main())\n",
        "```\n",
        "\n",
        "In the second example, `nest_asyncio.apply()` allows the `asyncio.run(main())` call to work within the already active IPython event loop.\n",
        "\n",
        "**Therefore, `import nest_asyncio` and subsequently calling `nest_asyncio.apply()` make it possible to use `asyncio.run()` or create new event loops even when one is already active.**\n"
      ],
      "metadata": {
        "id": "ZPov0iXtbwiV"
      },
      "id": "ZPov0iXtbwiV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dotenv\n",
        "\n",
        "The line `from dotenv import load_dotenv` imports the `load_dotenv` function from the `python-dotenv` library.\n",
        "\n",
        "**What `load_dotenv` does:**\n",
        "\n",
        "The `load_dotenv()` function reads key-value pairs from a `.env` file (if it exists in the current directory or a specified path) and sets them as environment variables in your Python process.\n",
        "\n",
        "This is a common practice for managing configuration settings, especially sensitive information like API keys, database credentials, and other environment-specific variables, separately from your main codebase. Instead of hardcoding these values, you store them in the `.env` file, and `load_dotenv()` makes them accessible through `os.environ`.\n",
        "\n",
        "**Why use `.env` files?**\n",
        "\n",
        "  * **Security:** Avoids hardcoding sensitive information in your code, making it safer to share or store in version control.\n",
        "  * **Configuration Management:** Allows you to easily change configuration based on the environment (e.g., development, testing, production) by using different `.env` files.\n",
        "  * **Clean Code:** Separates configuration from the application logic, making the code cleaner and easier to manage.\n",
        "\n",
        "**Example of a `.env` file:**\n",
        "\n",
        "Create a file named `.env` in the same directory as your Python script (or the directory where you intend to run it). Here's an example of what it might contain:\n",
        "\n",
        "```\n",
        "API_KEY=your_super_secret_api_key\n",
        "DATABASE_URL=postgresql://user:password@host:port/database\n",
        "DEBUG=True\n",
        "USER_ID=123\n",
        "```\n",
        "\n",
        "Each line in the `.env` file defines an environment variable in the format `KEY=VALUE`.\n",
        "\n",
        "**Example of how to use `load_dotenv()` in Python:**\n",
        "\n",
        "```python\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Access the environment variables\n",
        "api_key = os.environ.get(\"API_KEY\")\n",
        "database_url = os.environ.get(\"DATABASE_URL\")\n",
        "debug_mode = os.environ.get(\"DEBUG\")\n",
        "user_id = os.environ.get(\"USER_ID\")\n",
        "\n",
        "print(f\"API Key: {api_key}\")\n",
        "print(f\"Database URL: {database_url}\")\n",
        "print(f\"Debug Mode: {debug_mode}\")\n",
        "print(f\"User ID: {user_id}\")\n",
        "```\n",
        "\n",
        "When you run this Python script, `load_dotenv()` will read the `.env` file, and the `os.environ.get()` calls will retrieve the values you defined in it, if needed.\n"
      ],
      "metadata": {
        "id": "ip0xWNuRcJME"
      },
      "id": "ip0xWNuRcJME"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logfire\n",
        "\n",
        "```python\n",
        "import logfire\n",
        "logfire.configure(send_to_logfire='if-token-present')\n",
        "```\n",
        "\n",
        "1.  **`import logfire`**: This line imports the `logfire` library, making its functions and classes available for use in your Python script. The `logfire` library is designed for real-time debugging, performance monitoring, and behavior tracking of LLM-powered applications.\n",
        "\n",
        "2.  **`logfire.configure(send_to_logfire='if-token-present')`**: This line calls the `configure` function within the `logfire` library. The argument `send_to_logfire='if-token-present'` specifies a setting for when log data should be sent to the Logfire service.\n",
        "\n",
        "      * **`send_to_logfire`**: This is a parameter that controls whether and when log data is transmitted to the Logfire platform.\n",
        "      * **`'if-token-present'`**: This value tells `logfire` to send log data **only if** a Logfire API token has been configured (typically via an environment variable or another configuration method). If no token is found, the log data will likely be handled locally or not sent at all.\n",
        "\n",
        "**In summary, this code snippet initializes the `logfire` library and configures it to send logging, tracing, and potentially other monitoring data to the Logfire service, but only if a Logfire API token is available in the environment.** This is a common way to enable remote logging and monitoring when you have set up your Logfire account and provided the necessary credentials, while avoiding sending data unnecessarily if you haven't.\n",
        "\n",
        "To fully utilize Logfire, you would typically also use its logging functions (which might integrate with Python's standard `logging` module or provide their own) and potentially tracing functionalities within your application code.\n"
      ],
      "metadata": {
        "id": "cQGbNZnFcu5T"
      },
      "id": "cQGbNZnFcu5T"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}